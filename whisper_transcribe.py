import torch
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from scipy.io import wavfile
import numpy as np

# üéØ –ó–∞–≥–≤–∞—Ä—ã–Ω –±–∞–π—Ä–ª–∞–ª (latest/ —Ñ–æ–ª–¥–µ—Ä —Ä—É—É —á–∏–≥–ª“Ø“Ø–ª—Å—ç–Ω –±–∞–π—Ö —ë—Å—Ç–æ–π)
MODEL_DIR = "/home/billy/ai_voice_server/my_models"
SAMPLE_RATE = 16000

# üìå –ê—É–¥–∏–æ–Ω–æ–æ—Å —Ö”©—Ä–≤“Ø“Ø–ª—ç–ª—Ç —Ö–∏–π—Ö —Ñ—É–Ω–∫—Ü
def transcribe_audio(audio):
    processor = WhisperProcessor.from_pretrained(MODEL_DIR)
    model = WhisperForConditionalGeneration.from_pretrained(MODEL_DIR)

    model.eval()
    model.generation_config.forced_decoder_ids = None

    if torch.cuda.is_available():
        model.to("cuda")

    # üß† Input features –±—ç–ª—Ç–≥—ç—Ö
    inputs = processor(audio, sampling_rate=SAMPLE_RATE, return_tensors="pt")

    if torch.cuda.is_available():
        inputs = {k: v.to("cuda") for k, v in inputs.items()}

    # üîç No gradient
    with torch.no_grad():
        output = model.generate(
            inputs["input_features"],
            max_length=448,
            pad_token_id=processor.tokenizer.pad_token_id,
            decoder_start_token_id=processor.tokenizer.bos_token_id
        )

    text = processor.batch_decode(output, skip_special_tokens=True)[0]
    return text.strip()

# üìÅ WAV —Ñ–∞–π–ª —Ö”©—Ä–≤“Ø“Ø–ª—ç—Ö
def transcribe_audio_from_file(file_path):
    rate, audio = wavfile.read(file_path)

    # üéß WAV-–≥ float —Ä—É—É —Ö”©—Ä–≤“Ø“Ø–ª—ç—Ö (16-bit WAV –≥—ç–∂ “Ø–∑–Ω—ç)
    if audio.dtype == np.int16:
        audio = audio.astype(np.float32) / 32768.0
    elif audio.dtype == np.float32:
        audio = np.clip(audio, -1.0, 1.0)  # –∞–ª—å —Ö—ç–¥–∏–π–Ω OK
    else:
        raise ValueError(f"Unsupported audio dtype: {audio.dtype}")

    return transcribe_audio(audio)
